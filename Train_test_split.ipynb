{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split\n",
    "We want to withhold a subset of the data to estimate the performance of the final chosen model at the end. Therefore, we currently exclude 20% of the data from any analyses. All training, validation, and model comparison will be completed on the remaining 80% of the data.  \n",
    " \n",
    "Since the full asylum version is a subset of any asylum version, we create the split on the full asylum version, save the list of train and test cases,so that we can use this list to split the data for subsequent analyses.\n",
    "this code only needs to be run one time--once we've done it, we can load the train and test cases from the file where they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "pd.set_option('precision', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN CLEANED DATASET\n",
    "#path = '/home/emilyboeke/'\n",
    "#master_app = pd.read_csv(path + 'merged_any_master_app.csv', low_memory=False)\n",
    "master_app = pd.read_csv('/data/WorkData/spatialtemporal/finalmerge_any_latev2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_train, obs_test = train_test_split(master_app,  test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: nat\n",
      "train data: 188 categories.\n",
      "test data: 186 categories.\n",
      "Feature: case_type\n",
      "train data: 5 categories.\n",
      "test data: 6 categories.\n",
      "Feature: hearing_loc_code\n",
      "train data: 183 categories.\n",
      "test data: 181 categories.\n",
      "Feature: osc_date\n",
      "train data: 10508 categories.\n",
      "test data: 9572 categories.\n",
      "Feature: comp_date\n",
      "train data: 7141 categories.\n",
      "test data: 6877 categories.\n",
      "Feature: hearing_city\n",
      "train data: 120 categories.\n",
      "test data: 119 categories.\n",
      "Feature: adj_date\n",
      "train data: 7229 categories.\n",
      "test data: 6928 categories.\n",
      "Feature: sched_type\n",
      "train data: 20 categories.\n",
      "test data: 16 categories.\n",
      "Feature: lang_hearing\n",
      "train data: 310 categories.\n",
      "test data: 222 categories.\n",
      "Feature: notice_desc\n",
      "train data: 142 categories.\n",
      "test data: 93 categories.\n",
      "Feature: adj_time_start2\n",
      "train data: 49529 categories.\n",
      "test data: 26536 categories.\n",
      "Feature: adj_time_stop2\n",
      "train data: 59312 categories.\n",
      "test data: 31076 categories.\n",
      "Feature: adj_rsn_desc\n",
      "train data: 66 categories.\n",
      "test data: 50 categories.\n"
     ]
    }
   ],
   "source": [
    "#check that all feature values that are present in the test set are also present in the training set.\n",
    "categorical = obs_train.select_dtypes(include=['object'])\n",
    "for i in categorical.columns.values.tolist():\n",
    "    if obs_train[i].nunique() != obs_test[i].nunique():\n",
    "        print(\"Feature: \" + i)\n",
    "        print(\"train data: \"+ str(obs_train[i].nunique())+\" categories.\")\n",
    "        print(\"test data: \"+str(obs_test[i].nunique())+\" categories.\")\n",
    "        \n",
    "        #np.where(test[i].isin(train[i]), 'found in train set', test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save a list of train and test idncases.\n",
    "train_cases = obs_train.idncase\n",
    "test_cases = obs_test.idncase\n",
    "train_cases.to_csv('/data/WorkData/spatialtemporal/train_cases_last_hearingv2.csv',index=False)\n",
    "test_cases.to_csv('/data/WorkData/spatialtemporal/test_cases_last_hearingv2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
