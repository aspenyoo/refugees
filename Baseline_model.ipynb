{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline_model.ipynb\n",
    "The goal of this model is to predict whether a refugee will be granted asylum based on information available before the actual hearing. For this baseline model, we fit an L2 logistic regression using three variables of interest, described below.\n",
    "\n",
    "#### Features  \n",
    "   * `nat`: applicant's nationality (one hot encoded)\n",
    "   * `tracid`: judge's identification number (one hot encoded)\n",
    "   * `osc_date`: the Notice to Appear date for each proceeding (continuous)  \n",
    "  \n",
    "<br>\n",
    "We define 'asylum granted' in two different ways. These slightly different definitions require different cleaning of the data, which lead to the following two datasets we use in the baseline model.\n",
    "1. `merged_full_asylum_master_app.csv`: individual (indicated by unique idnProceeding) is granted Full Asylum. \n",
    "2. `merged_any_master_app.csv`: individual is granted at least one of the following: Full Asylum, Withholding of Removal, Protection Under Convention Against Torture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format features\n",
    "Variables of interest had been cleaned and merged in Cleaning.ipynb. However, these features need to be transformed into the appropriate format (e.g., one hot encoded) for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "pd.set_option('precision', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN CLEANED DATASET\n",
    "path = '/home/emilyboeke/'\n",
    "\n",
    "master_app = pd.read_csv(path + 'merged_any_master_app.csv', low_memory=False)\n",
    "#master_app = pd.read_csv(path + 'merged_full_asylum_master_app.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the idncases in the training set, and select the rows of master_app that correspond to those cases.\n",
    "train_cases = pd.read_csv(path + 'train_cases_any_asylum.csv',header=None).values\n",
    "train_cases = train_cases.reshape(train_cases.shape[0])\n",
    "master_app = master_app[master_app.idncase.isin(train_cases)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare variables to go into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE OSC_DATE TO CONTINUOUS NUMBER (number of days since startdate)\n",
    "\n",
    "master_app['osc_date'] = pd.to_datetime(master_app['osc_date'],infer_datetime_format = True)\n",
    "osc_date_cont = []\n",
    "startdate = np.datetime64('1984-01-01') # earliest date. from which timedelta is calculated\n",
    "\n",
    "# change osc_date to continuous number\n",
    "for i in master_app.index:\n",
    "    x = master_app.loc[i,'osc_date'] - startdate\n",
    "    osc_date_cont.append(x.days)\n",
    "    \n",
    "osc_date_cont = np.array(osc_date_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change dec to binary variable\n",
    "master_app.loc[(master_app[\"dec\"] == 'DENY'),'dec'] = 0\n",
    "master_app.loc[(master_app[\"dec\"] == 'GRANT'),'dec'] = 1\n",
    "\n",
    "y = master_app.dec\n",
    "#master_app.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODE CATEGORICAL VARIABLES\n",
    "\n",
    "# change string nationalities to integer categories \n",
    "le = LabelEncoder()\n",
    "nat_int = le.fit_transform(master_app['nat'])\n",
    "nat_int = np.reshape(nat_int,[len(nat_int),1])\n",
    "\n",
    "# get N x 2 array of features of interest\n",
    "feat_int = np.concatenate((master_app[['tracid']], nat_int), axis=1)\n",
    "feat_int.shape\n",
    "\n",
    "# get one hot encoder of features\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(feat_int)  \n",
    "enc.feature_indices_\n",
    "\n",
    "# create sparse matrix of all observations in Compressed Sparse Row format\n",
    "blah = enc.transform(feat_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCATENATE ONE HOT ENCODED FEATURES WITH CONTINUOUS FEATURE\n",
    "\n",
    "# changes csr to csc, bc simpler to work with columns than rows\n",
    "blah = scipy.sparse.csr_matrix.tocsc(blah)\n",
    "# concatenating relevant fields \n",
    "new_data = np.concatenate((blah.data, osc_date_cont)) # non-zero values in matrix\n",
    "new_indices = np.concatenate((blah.indices, range(len(osc_date_cont)))) # row indices for each column\n",
    "new_ind_ptr = np.append(blah.indptr, blah.indptr[-1]+len(osc_date_cont))\n",
    "# making new matrix\n",
    "X = csc_matrix((new_data, new_indices, new_ind_ptr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement L2 logistic regression\n",
    "\n",
    "We use sklearn to define a logistic regression with an L2 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(penalty='l2') # defining model: logistic regression with L2 penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement model on train/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7307730618117426\n"
     ]
    }
   ],
   "source": [
    "# split data into train and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = LogReg.fit(X_train, y_train)\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the accuracy of predicting grant rate just from the applicant's nationality, the judge's ID, and the Notice to Appear date is sufficient to predict the following for each data set:  \n",
    "  * `merged2_master_app.csv`: around 73% accuracy\n",
    "  * `merged_any_master_app.csv`: around 73.3% accuracy\n",
    "  * `merged_full_asylum_master_app.csv`: around 73.5% accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.888390763926314\n",
      "0.9809239771939438\n",
      "-0.004849619656256326\n"
     ]
    }
   ],
   "source": [
    "# print some summary stats on the weights\n",
    "coeffs = (model.coef_[0])\n",
    "\n",
    "print(min(coeffs))\n",
    "print(max(coeffs))\n",
    "print(np.mean(coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(coeffs, bins='auto') \n",
    "plt.title(\"L2 logistic regression weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOK AT LARGEST MAGNITUDE WEIGHTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model using k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64945327 0.69279295 0.64181681 0.70449565 0.72379751 0.71279633\n",
      " 0.71461142 0.71253071 0.78004294 0.64713565]\n",
      "0.6979473251269621\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogReg, X, y, scoring='accuracy', cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
